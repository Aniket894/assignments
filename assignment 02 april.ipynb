{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6a6e4a-a110-42d6-b9db-3780042db4f2",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78cd86-2f37-4aa2-b451-0ae326da85c1",
   "metadata": {},
   "source": [
    "ans  - The purpose of grid search cross-validation (GridSearchCV) in machine learning is to systematically search for the optimal hyperparameters of a model. Hyperparameters are settings or configurations that are not learned from the data, but rather set by the user before training the model. Examples of hyperparameters include the learning rate of an algorithm, the number of hidden layers in a neural network, or the regularization parameter in a support vector machine.\n",
    "\n",
    "GridSearchCV works by exhaustively evaluating a specified set of hyperparameter combinations to determine the best configuration for the model. It performs a grid search over all possible combinations of hyperparameters by creating a Cartesian product of all parameter values provided. For each combination, the model is trained and evaluated using cross-validation.\n",
    "\n",
    "Here are the steps involved in the GridSearchCV process:\n",
    "\n",
    "Define the model: Select the algorithm or model to be tuned, along with the hyperparameters that need to be optimized.\n",
    "\n",
    "Define the parameter grid: Specify the range or values for each hyperparameter that you want to search. These values are typically provided as a dictionary or a list of dictionaries, where each dictionary represents a combination of hyperparameters.\n",
    "\n",
    "Cross-validation: Split the training data into multiple folds or subsets. For each combination of hyperparameters, the model is trained on a portion of the data (training set) and evaluated on the remaining part (validation set). This process is repeated for each fold, and the performance is averaged.\n",
    "\n",
    "Evaluation: Determine a scoring metric, such as accuracy, precision, recall, or F1-score, to evaluate the model's performance for each combination of hyperparameters.\n",
    "\n",
    "Grid search: Perform the grid search by iterating through all combinations of hyperparameters. Each combination is evaluated using cross-validation, and the average performance metric is recorded.\n",
    "\n",
    "Best hyperparameters: Once the grid search is complete, the hyperparameter combination that yields the best performance metric is identified.\n",
    "\n",
    "Retrain the model: Finally, the model is trained on the complete training dataset using the best hyperparameters obtained from the grid search.\n",
    "\n",
    "GridSearchCV automates the process of hyperparameter tuning, enabling a systematic search for optimal configurations. It helps to find the hyperparameters that result in the best model performance without the need for manual trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94718623-df22-4bf8-ab5a-86d31c53699c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a138ad9-c8a1-457f-86e5-151659a6e798",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca67cf-e1de-488a-9782-4cad8e325d0c",
   "metadata": {},
   "source": [
    "ans - \n",
    "Grid search cross-validation (GridSearchCV) and randomized search cross-validation (RandomizedSearchCV) are both techniques used for hyperparameter tuning in machine learning. While they serve the same purpose of finding the optimal hyperparameters, they differ in how they search through the hyperparameter space.\n",
    "\n",
    "The main difference between GridSearchCV and RandomizedSearchCV lies in their search strategies:\n",
    "\n",
    "GridSearchCV: Grid search exhaustively evaluates all possible combinations of hyperparameters specified in a predefined grid. It creates a Cartesian product of all parameter values and trains the model for each combination. This means that GridSearchCV explores the entire search space systematically.\n",
    "\n",
    "RandomizedSearchCV: Randomized search, on the other hand, randomly samples a fixed number of hyperparameter combinations from a specified distribution. It selects the values for each hyperparameter independently, and each combination is evaluated. RandomizedSearchCV provides more flexibility in terms of the number of iterations and the range of hyperparameter values to consider.\n",
    "\n",
    "When to choose GridSearchCV:\n",
    "\n",
    "When the search space of hyperparameters is relatively small and it is feasible to exhaustively evaluate all combinations.\n",
    "When computational resources are not a limitation.\n",
    "When you want to ensure that you have explored the entire search space and want to find the absolute best hyperparameter combination.\n",
    "When to choose RandomizedSearchCV:\n",
    "\n",
    "When the search space of hyperparameters is large, making it impractical to evaluate all possible combinations.\n",
    "When computational resources are limited and you want to reduce the overall search time.\n",
    "When you are unsure about the best values or range for hyperparameters and want to explore a wide range of values randomly.\n",
    "When there is a possibility that some hyperparameters may not significantly impact the model's performance, and random sampling can efficiently identify important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d6c02-30b0-44dc-bbbc-e6a7f6762c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "080a92f2-03b6-4b1b-aa3a-31bbea8c2148",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f826f9-9699-4185-979a-7762d69a173a",
   "metadata": {},
   "source": [
    "ans - Data leakage, in the context of machine learning, refers to the situation where information from outside the training data \"leaks\" into the model during the training process, resulting in overly optimistic performance estimates. It occurs when the model has access to information that it would not have in a real-world scenario or when the model is inadvertently influenced by information that should be unknown at the time of prediction.\n",
    "\n",
    "Data leakage is a problem in machine learning because it leads to overfitting, where the model performs well on the training data but fails to generalize to new, unseen data. This can lead to misleading results, inflated performance metrics, and poor performance in real-world applications. Data leakage undermines the ability of the model to make accurate predictions on new, unseen data.\n",
    "\n",
    "Here's an example to illustrate data leakage:\n",
    "\n",
    "Suppose you are building a credit risk model to predict whether a loan applicant will default on their loan. You have a dataset with various features, including the applicant's income, credit score, employment history, and previous loan payment records.\n",
    "\n",
    "Now, imagine that you mistakenly include the loan repayment status as a feature in the dataset. In other words, you provide the model with information that it should not have at the time of prediction because the repayment status is determined after the loan has been granted.\n",
    "\n",
    "During the training process, the model inadvertently learns the relationship between the loan repayment status and the target variable (default or not). As a result, it becomes overly optimistic and performs well during training because it has access to future information that it should not have in practice.\n",
    "\n",
    "However, when the model is deployed and used to make predictions on new loan applications, it will not have access to the loan repayment status because it is an unknown factor. Consequently, the model's performance will likely be poor because it has not learned to generalize based on the available features at the time of prediction.\n",
    "\n",
    "This example demonstrates how data leakage can occur when the model is exposed to information that is not genuinely available during prediction, leading to misleading results and unreliable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e18cd2-27bf-4f2f-8aaa-1766000a41d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7079dc2d-eac8-4b01-b85d-09a08946947c",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba628055-251d-4f5e-bab6-137a0eebe1d2",
   "metadata": {},
   "source": [
    "ans - Preventing data leakage is crucial when building machine learning models to ensure the integrity and reliability of the results. Here are some important practices to prevent data leakage:\n",
    "\n",
    "Understanding the data: Gain a deep understanding of the dataset and the problem domain. This involves studying the data collection process, data fields, and potential sources of data leakage.\n",
    "\n",
    "Splitting data properly: Split the dataset into separate sets for training, validation, and testing. Data leakage can occur if information from the validation or test sets accidentally leaks into the training set.\n",
    "\n",
    "Maintaining a strict separation: Ensure a strict separation of data between different stages of model development. For example, avoid using future information during the feature engineering phase, as it might not be available during real-world predictions.\n",
    "\n",
    "Avoiding target leakage: Target leakage occurs when information that would not be available in practical scenarios is inadvertently included in the features. For instance, using future information to predict a past event. Carefully engineer features to exclude any information that may lead to target leakage.\n",
    "\n",
    "Handling temporal data: If dealing with time-series or sequential data, ensure that the data split follows the temporal order. For example, if predicting future events, split the data to have past data for training and future data for testing.\n",
    "\n",
    "Removing sensitive or irrelevant features: Exclude sensitive or irrelevant features from the dataset that may introduce bias or leak sensitive information.\n",
    "\n",
    "Applying feature scaling: Perform feature scaling (e.g., normalization or standardization) to avoid leaking information between features with different scales or units.\n",
    "\n",
    "Applying proper cross-validation: Utilize appropriate cross-validation techniques, such as k-fold cross-validation, while ensuring that the validation process does not contaminate the training data.\n",
    "\n",
    "Regularization techniques: Incorporate regularization techniques, like L1 or L2 regularization, to prevent overfitting and improve model generalization. Regularization can help reduce the risk of data leakage by penalizing overly complex models.\n",
    "\n",
    "Constant monitoring: Continuously monitor the model's performance and evaluate for any unexpected patterns or inconsistencies that might indicate data leakage. Regularly audit the data and model pipeline to ensure data integrity.\n",
    "\n",
    "It's important to note that the prevention of data leakage is an ongoing process throughout the entire machine learning workflow. Vigilance, careful analysis of the data, and adherence to best practices can help minimize the risk of data leakage and produce reliable machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6b04c-fd9c-4585-b805-d3f88265c6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c22ad1-9fcf-4b7a-b0e7-6bf1c8bb6493",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e56aa-3cf3-484f-b844-b76c7febb894",
   "metadata": {},
   "source": [
    "ans - A confusion matrix is a table that is used to evaluate the performance of a classification model. It provides a summary of the predictions made by the model on a set of test data, compared to the true labels of the data.\n",
    "\n",
    "The confusion matrix is commonly used in machine learning and data science to assess the accuracy and quality of a classification model, especially when dealing with binary or multi-class classification problems.\n",
    "\n",
    "The confusion matrix is typically represented as a square matrix with rows and columns representing the true and predicted classes, respectively. In a binary classification scenario, the matrix has two rows and two columns, but in multi-class classification, it can have more rows and columns.\n",
    "\n",
    "Here is an example of a binary classification confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a7dcb-bf0b-40fd-9f45-625a446bd5a4",
   "metadata": {},
   "source": [
    "                 Predicted Negative    Predicted Positive\n",
    "Actual Negative        TN (True Negative)     FP (False Positive)\n",
    "Actual Positive        FN (False Negative)    TP (True Positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73516661-e77e-4011-987e-1fdd6637d862",
   "metadata": {},
   "source": [
    "The elements of the confusion matrix represent the following:\n",
    "\n",
    "True Positive (TP): The model correctly predicted the positive class.\n",
    "True Negative (TN): The model correctly predicted the negative class.\n",
    "False Positive (FP): The model incorrectly predicted the positive class when the true class was negative. Also known as a Type I error.\n",
    "False Negative (FN): The model incorrectly predicted the negative class when the true class was positive. Also known as a Type II error.\n",
    "\n",
    "\n",
    "By analyzing the values in the confusion matrix, you can derive various performance metrics of the classification model, such as:\n",
    "\n",
    "Accuracy: It is the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "Precision: It measures the proportion of correctly predicted positive instances out of all instances predicted as positive, calculated as TP / (TP + FP).\n",
    "Recall (also called Sensitivity or True Positive Rate): It measures the proportion of correctly predicted positive instances out of all actual positive instances, calculated as TP / (TP + FN).\n",
    "Specificity (also called True Negative Rate): It measures the proportion of correctly predicted negative instances out of all actual negative instances, calculated as TN / (TN + FP).\n",
    "F1 Score: It is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
    "\n",
    "\n",
    "The confusion matrix helps in understanding the distribution of errors made by the model, identifying whether it is more prone to false positives or false negatives. It allows you to assess the trade-offs between precision and recall and make informed decisions about model adjustments or improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd88f51-e4c0-47f5-91c7-9adfdf82ab0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678a8aa8-0d23-44e5-a78a-5748d48b253a",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb6bdd-777e-4ed5-bca6-70b52e1b84c1",
   "metadata": {},
   "source": [
    "ans - In the context of a confusion matrix, precision and recall are two important metrics used to evaluate the performance of a classification model.\n",
    "\n",
    "Precision, also known as the positive predictive value, measures the accuracy of the model's positive predictions. It is calculated as the ratio of true positive (TP) predictions to the sum of true positives and false positives (FP). In simpler terms, precision quantifies how many of the positive predictions made by the model are actually correct. A higher precision indicates a lower number of false positives, meaning the model is more precise in identifying positive instances.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify all positive instances. It is calculated as the ratio of true positive predictions to the sum of true positives and false negatives (FN). Recall quantifies the model's effectiveness in capturing all the positive instances present in the data. A higher recall indicates a lower number of false negatives, meaning the model is better at not missing positive instances.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Both precision and recall have their own significance depending on the problem at hand. For example, in a spam email classifier, high precision is desired to minimize the number of legitimate emails marked as spam. On the other hand, in a disease detection model, high recall is more important to ensure minimal false negatives, so that no positive cases are missed.\n",
    "\n",
    "In practice, there is often a trade-off between precision and recall. Increasing one metric may lead to a decrease in the other. The F1 score, which is the harmonic mean of precision and recall, is commonly used to strike a balance between the two metrics and provide an overall evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59d9f4-dc4f-4af1-9264-412af06c7390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a232cd1-1c72-47b8-9fbb-96b0d67439c0",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124b294-60a8-4b6c-9d4a-39f249f38aa5",
   "metadata": {},
   "source": [
    "ans - A confusion matrix provides a tabular representation of the performance of a classification model, showing the predicted and actual class labels for a set of test data. It consists of four main components: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These components can help us interpret the types of errors the model is making. Here's how you can interpret them:\n",
    "\n",
    "True Positives (TP): These are the instances where the model predicted a positive class correctly, and the actual class is also positive. For example, in a medical diagnosis scenario, a true positive would represent a correctly identified disease.\n",
    "\n",
    "True Negatives (TN): These are the instances where the model predicted a negative class correctly, and the actual class is also negative. For instance, in a spam email classification task, a true negative would be an email correctly classified as non-spam.\n",
    "\n",
    "False Positives (FP): These are the instances where the model predicted a positive class incorrectly, while the actual class is negative. In the medical diagnosis example, a false positive would be when the model incorrectly identifies a healthy patient as having a disease.\n",
    "\n",
    "False Negatives (FN): These are the instances where the model predicted a negative class incorrectly, while the actual class is positive. In the spam email classification task, a false negative would occur when the model incorrectly classifies a spam email as non-spam.\n",
    "\n",
    "Interpreting these components allows you to gain insights into the types of errors your model is making:\n",
    "\n",
    "High false positives (FP) indicate that the model is incorrectly predicting positive instances. It may be prone to making false alarms or identifying non-existent patterns.\n",
    "\n",
    "High false negatives (FN) suggest that the model is missing positive instances. It may fail to identify certain patterns or has a tendency to overlook certain characteristics.\n",
    "\n",
    "By examining the confusion matrix, you can analyze the distribution of these errors and determine the specific types of mistakes your model is prone to making. This information can guide you in understanding the strengths and weaknesses of your model, and help you make improvements or take corrective actions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899e8d7-5152-42e7-9b01-f957e75b7699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a2e1695-b6da-49b4-a399-c594c9030f01",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14713a2f-3777-4ebe-b6af-facf78022a63",
   "metadata": {},
   "source": [
    "ans - Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. Here are some of the key metrics:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the model's predictions. It is calculated as the ratio of the sum of true positives (TP) and true negatives (TN) to the total number of instances.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: Precision, also called the positive predictive value, quantifies the accuracy of positive predictions. It is calculated as the ratio of TP to the sum of TP and false positives (FP).\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: Recall, also known as sensitivity or true positive rate, measures the model's ability to capture positive instances. It is calculated as the ratio of TP to the sum of TP and false negatives (FN).\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that considers both precision and recall.\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Specificity: Specificity, also known as the true negative rate, measures the model's ability to correctly identify negative instances. It is calculated as the ratio of TN to the sum of TN and FP.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "False Positive Rate (FPR): FPR measures the proportion of actual negatives that are incorrectly classified as positives. It is calculated as the ratio of FP to the sum of FP and TN.\n",
    "\n",
    "FPR = FP / (FP + TN)\n",
    "\n",
    "These metrics provide different perspectives on the model's performance. Accuracy gives an overall view, while precision and recall focus on positive predictions. F1 score provides a balanced evaluation, and specificity and FPR are relevant for specific scenarios where negative instances are of interest.\n",
    "\n",
    "It's important to note that the choice of metrics depends on the problem domain and the specific objectives of the classification task. Different metrics may be more suitable for different applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99871b-0a31-4631-9708-f58c27b3d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdc34f5-b728-4bb9-92b0-971c00a7faaf",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f94737-ffaf-4e32-81ef-4a1331e43a1b",
   "metadata": {},
   "source": [
    "ans - The accuracy of a model is closely related to the values in its confusion matrix. The confusion matrix provides a breakdown of the model's predictions and the actual class labels. By examining the values in the confusion matrix, we can calculate the accuracy of the model.\n",
    "\n",
    "Accuracy is defined as the ratio of the sum of true positives (TP) and true negatives (TN) to the total number of instances. In the context of a confusion matrix, accuracy can be calculated as follows:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "The true positives (TP) and true negatives (TN) represent the correctly predicted instances, while the false positives (FP) and false negatives (FN) represent the errors made by the model.\n",
    "\n",
    "The accuracy of the model indicates the overall correctness of its predictions. A higher accuracy means that the model is making more correct predictions, while a lower accuracy suggests that the model is making more errors.\n",
    "\n",
    "The values in the confusion matrix directly contribute to the accuracy calculation. By correctly classifying instances as true positives and true negatives, the model increases the numerator of the accuracy formula. Conversely, false positives and false negatives decrease the accuracy since they are errors made by the model.\n",
    "\n",
    "It's important to note that accuracy alone may not provide a complete picture of the model's performance, especially in imbalanced datasets where the classes have unequal representation. Therefore, it is often beneficial to consider additional metrics such as precision, recall, F1 score, and specificity to gain a more comprehensive understanding of the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5dcea-b0c2-405a-bdff-19bda617cd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98851ecc-6826-46ef-a25d-08713892d902",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a872589-1a22-4332-9ad4-a97af431652a",
   "metadata": {},
   "source": [
    "ans - A confusion matrix can be a valuable tool for identifying potential biases or limitations in a machine learning model. By examining the distribution of predictions and actual class labels, we can gain insights into how the model performs across different classes and identify areas of concern. Here are some steps to use a confusion matrix for this purpose:\n",
    "\n",
    "Class Imbalance: Check if there is a significant difference in the number of instances between different classes. If one class dominates the dataset, the model may be biased towards that class, leading to imbalanced predictions.\n",
    "\n",
    "False Positives and False Negatives: Examine the counts of false positives (FP) and false negatives (FN) for each class. Look for any disparities in error rates across classes. Higher false positives indicate a higher likelihood of incorrectly predicting the positive class, while higher false negatives suggest a higher chance of missing positive instances.\n",
    "\n",
    "Error Patterns: Analyze the specific types of errors made by the model. Are there certain classes that are consistently misclassified? This can indicate limitations in the model's ability to distinguish between similar classes or handle specific patterns.\n",
    "\n",
    "Sensitivity to Class Distribution: Assess how the model's performance varies with changes in the class distribution. Randomly sample instances from different classes and evaluate the model's predictions. If the model's performance varies significantly across different data subsets, it may indicate sensitivity to class imbalances or biases in the training data.\n",
    "\n",
    "Bias in Predictions: Check if the model exhibits any systematic biases or tends to favor certain classes over others. This can be observed by comparing the precision and recall values for different classes. Significant variations in these metrics suggest potential biases in the model's predictions.\n",
    "\n",
    "External Factors: Consider external factors or domain-specific knowledge that may influence the model's performance. For example, if the model shows lower performance on certain demographics, it could indicate bias or limitations in the training data or features used by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
