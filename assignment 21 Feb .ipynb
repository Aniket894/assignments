{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fda0328-2c95-47f2-959b-5972d42ed4b0",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e0972-4643-4a35-9c12-921b2b87a598",
   "metadata": {},
   "source": [
    "ans - Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664fe4b-4f3c-49f0-b0e3-476df004685c",
   "metadata": {},
   "source": [
    "when we want a paragraph on Donald Trump! What do you do? Well, you can copy and paste the information from Wikipedia to your own file. But what if you want to get large amounts of information from a website as quickly as possible? Such as large amounts of data from a website to train a Machine Learning algorithm? In such a situation, copying and pasting will not work! And that’s when you’ll need to use Web Scraping.Unlike the long and mind-numbing process of manually getting data, Web scraping uses intelligence automation methods to get thousands or even millions of data sets in a smaller amount of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acebaf-d9fe-40d3-9e31-c704224dc42c",
   "metadata": {},
   "source": [
    "Web Scraping has multiple applications across various industries :- \n",
    "\n",
    "1. Price Monitoring - \n",
    "\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research\n",
    "\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring\n",
    "\n",
    "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d8fac-b0c3-478b-97b8-4b6b76bbe0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7cb15c0-7af9-4cf1-a4de-542c2917510c",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b5bce-c7d8-40ba-a166-55e2608681a3",
   "metadata": {},
   "source": [
    "ans - There are many methods used in web scrapping  -\n",
    "\n",
    "Text Pattern Matching - The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "\n",
    "HTTP Programming - Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
    "\n",
    "HTML Parsing - Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "\n",
    "Wrapper - generation algorithms assume that the input pages of a wrapper induction system follow a common template and can be identified using a URL common scheme. Furthermore, semi-structured data query languages such as XQuery and HTQL can be used to parse HTML pages as well as retrieve and transform page content.\n",
    "\n",
    "DOM Parsing - Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
    "\n",
    "Vertical Aggregation - Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
    "\n",
    "The robustness of the platform is measured by the quality of the information it retrieves (typically the number of fields) and its scalability (how quickly it can scale up to hundreds or thousands of sites). This scalability is primarily used to target the Long Tail of sites that common aggregators find too difficult or time-consuming to harvest content from.\n",
    "\n",
    "Semantic Annotation Recognizing - The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
    "\n",
    "Computer Vision Web-Page Analysis - There are efforts using machine learning and computer vision to identify and extract information from web pages by visually interpreting pages as a human would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c09f73-a410-4b9d-a248-bc3f5007527d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840d5045-4c67-4fd3-9f5b-ca15f712b437",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58d104-9782-4f88-8d1e-4ce5d845c58d",
   "metadata": {},
   "source": [
    "ans -BeautifulSoup object is provided by Beautiful Soup which is a web scraping framework for Python. Web scraping is the process of extracting data from the website using automated tools to make the process faster. The BeautifulSoup object represents the parsed document as a whole. It beatify the html file.It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f3364-f846-4a43-be4f-8aa2208266fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628e2cd0-a03a-400a-8699-4a0c38ac6c96",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d68f0c-093c-4d38-a2eb-2a8031ad189a",
   "metadata": {},
   "source": [
    "ans - Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b210509-1b2e-4f1a-921b-5e84ba8681ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239ab241-2e54-46a0-85df-5f338e5d846c",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25436a9a-494b-4fa6-bb94-bc2b108b8e59",
   "metadata": {},
   "source": [
    "ans - Codepipline and Elastic Beanstalk is used in deployment in aws.\n",
    "\n",
    "Elastic Beanstalk : Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\n",
    "\n",
    "Codepipline : AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
