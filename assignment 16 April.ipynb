{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67d7acb-f81a-4bd6-afc8-d79875ef0ef9",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca83cf-9749-471d-9a0c-7bd776b99d4c",
   "metadata": {},
   "source": [
    "ans - Boosting is a machine learning ensemble technique that combines multiple weak learners to create a strong learner. It is a sequential learning process where each weak learner is trained to correct the mistakes made by the previous weak learners. The main idea behind boosting is to create a powerful model by aggregating the predictions of several individual models, each focusing on different aspects of the data.\n",
    "\n",
    "In boosting, weak learners refer to models that are slightly better than random guessing, such as decision trees with limited depth or simple rules. These weak learners are trained on different subsets of the training data, where each subset is generated by giving more weight to the instances that were misclassified by the previous learners. This iterative process allows subsequent learners to focus more on the difficult instances and improve the overall accuracy of the ensemble.\n",
    "\n",
    "Boosting algorithms, such as AdaBoost (Adaptive Boosting) and Gradient Boosting, assign weights to each weak learner based on their performance. Weak learners that make fewer mistakes or have higher accuracy are given higher weights, and their predictions contribute more to the final ensemble model. This way, boosting combines the strengths of multiple weak learners to create a strong, robust model capable of making accurate predictions on new, unseen data.\n",
    "\n",
    "Boosting is particularly effective in situations where a single model may struggle to capture the complexity of the data or where there is a high level of noise or variability. It has been successfully applied to various machine learning tasks, including classification, regression, and ranking problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55823e00-8290-4a03-90a1-5118154e7cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19fb23f7-5218-4220-9ade-e7d91e8b6de1",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e09222-8c3c-469f-ae52-222d1201b16d",
   "metadata": {},
   "source": [
    "ans - Boosting techniques in machine learning offer several advantages, but they also have some limitations. Let's discuss them:\n",
    "\n",
    "Advantages of Boosting:\n",
    "\n",
    "Improved Predictive Accuracy: Boosting algorithms can significantly improve the predictive accuracy compared to using a single model or other ensemble methods. By combining multiple weak learners, boosting can effectively capture complex patterns and relationships in the data, leading to better predictions.\n",
    "\n",
    "Handling of Complex Data: Boosting algorithms can handle complex datasets with high noise or variability. They are robust against outliers and can adapt to nonlinear relationships, making them suitable for a wide range of machine learning tasks.\n",
    "\n",
    "Feature Importance: Boosting algorithms provide a measure of feature importance, allowing users to understand which features contribute the most to the final predictions. This information can be valuable for feature selection and understanding the underlying factors driving the predictions.\n",
    "\n",
    "Avoidance of Overfitting: Boosting algorithms employ techniques like weight adjustments and regularization to prevent overfitting. By iteratively focusing on misclassified instances, boosting reduces the bias and variance of the ensemble model, leading to better generalization performance.\n",
    "\n",
    "Limitations of Boosting:\n",
    "\n",
    "Sensitivity to Noisy Data and Outliers: Boosting algorithms can be sensitive to noisy data and outliers in the training set. As boosting assigns higher weights to misclassified instances, it may overemphasize the impact of noisy or outlier examples, leading to reduced performance.\n",
    "\n",
    "Computational Complexity: Boosting algorithms require sequential training of multiple weak learners, which can be computationally expensive and time-consuming, especially for large datasets. The training time increases with the number of iterations and the complexity of the weak learners.\n",
    "\n",
    "Potential for Overfitting: Although boosting algorithms aim to reduce overfitting, if the weak learners become too complex or the number of iterations is too high, there is a risk of overfitting the training data. Careful parameter tuning and regularization techniques are necessary to avoid this issue.\n",
    "\n",
    "Lack of Interpretability: Boosting models are often considered black boxes, making it challenging to interpret and explain the reasoning behind their predictions. While feature importance measures can provide some insights, the inner workings of boosting models can be difficult to interpret compared to simpler models like decision trees.\n",
    "\n",
    "It's important to consider these advantages and limitations when deciding to use boosting techniques in machine learning. The specific characteristics of the data and the requirements of the problem at hand should guide the choice of appropriate algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d8f45-b540-4c38-ad71-adc6f7ebf936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e311b0-2e00-46b3-b6e3-d438a2ae7962",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc53749-66b3-4e64-9989-402068b97de9",
   "metadata": {},
   "source": [
    "ans - Boosting is a sequential ensemble learning technique that combines multiple weak learners to create a strong learner. The main idea behind boosting is to iteratively train weak learners on different subsets of the training data, where each subset is modified to emphasize the instances that were misclassified by the previous weak learners.\n",
    "\n",
    "Here is a step-by-step explanation of how boosting works:\n",
    "\n",
    "Initialization: Initially, each instance in the training set is assigned equal weights. These weights represent the importance of each instance during the training process.\n",
    "\n",
    "Iterative Training: Boosting consists of multiple iterations, and at each iteration, a weak learner is trained on a modified version of the training data. The modifications are based on the performance of the previous weak learners.\n",
    "\n",
    "Weighted Training: In each iteration, the weak learner focuses on the instances that were misclassified by the previous weak learners. The training data for each iteration is weighted, with higher weights given to the misclassified instances. This adjustment ensures that subsequent weak learners pay more attention to the difficult instances.\n",
    "\n",
    "Weak Learner Training: A weak learner, often a simple model like a decision tree or a rule, is trained on the weighted training data. The weak learner's objective is to minimize the weighted training error by finding the best split or rule that separates the instances.\n",
    "\n",
    "Weight Update: After the weak learner is trained, its performance is evaluated on the training data. The instances that were misclassified receive higher weights, indicating their importance for the next iteration. The weights of correctly classified instances may be decreased or remain the same.\n",
    "\n",
    "Combining Weak Learners: The weak learner's predictions are combined with the predictions of the previously trained weak learners using a weighted sum. The weights of the weak learners are determined based on their performance in the training process. More accurate weak learners typically have higher weights, indicating their contribution to the final ensemble model.\n",
    "\n",
    "Iteration Termination: The boosting process continues for a fixed number of iterations or until a predefined stopping criterion is met. Common stopping criteria include reaching a certain level of accuracy or when further iterations do not improve the performance.\n",
    "\n",
    "Final Ensemble Prediction: The predictions of all weak learners are combined to form the final ensemble prediction. The specific combination method depends on the boosting algorithm used. For example, AdaBoost calculates the weighted majority vote of the weak learners, while Gradient Boosting performs an additive combination using the weak learners' predictions.\n",
    "\n",
    "By combining the predictions of multiple weak learners, boosting creates a strong ensemble model that can make accurate predictions on new, unseen data. The process of iteratively adjusting the weights and training new weak learners allows boosting to focus on difficult instances and capture complex patterns in the data, leading to improved predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aac1b7-5dbc-4cb8-9d9b-9a208d48c77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141c738c-872e-4eef-8a8f-12e2b6431454",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24095000-612b-4ced-bcec-566c36a600ec",
   "metadata": {},
   "source": [
    "ans - There are several types of boosting algorithms, each with its own characteristics and variations. Here are some of the commonly used boosting algorithms:\n",
    "\n",
    "AdaBoost (Adaptive Boosting): AdaBoost is one of the earliest and most popular boosting algorithms. In each iteration, AdaBoost adjusts the weights of the training instances based on their misclassification. It assigns higher weights to misclassified instances, allowing subsequent weak learners to focus on them. AdaBoost also assigns weights to the weak learners themselves, depending on their performance, and combines their predictions through a weighted majority vote.\n",
    "\n",
    "Gradient Boosting: Gradient Boosting is a general framework that can be used with various loss functions and weak learner types. It builds the ensemble model in an additive manner by sequentially fitting weak learners to the negative gradient of the loss function. Each new weak learner is trained to minimize the residual errors of the previous ensemble predictions. Gradient Boosting variants include XGBoost (Extreme Gradient Boosting) and LightGBM (Light Gradient Boosting Machine), which introduce additional optimization techniques for improved performance.\n",
    "\n",
    "Stochastic Gradient Boosting: Stochastic Gradient Boosting, also known as Gradient Boosting with Randomness, introduces randomness into the training process. It randomly samples subsets of the training data and features for each iteration, which helps to reduce overfitting and enhance generalization. Variants of stochastic gradient boosting include Random Forests, which combine gradient boosting with random feature subsetting, and Stochastic Gradient Boosting with Early Stopping, which terminates the boosting process when performance on a validation set no longer improves.\n",
    "\n",
    "CatBoost: CatBoost is a boosting algorithm that handles categorical features more effectively than other algorithms. It employs an innovative approach called Ordered Boosting, which uses the natural ordering of categorical features to convert them into numerical values. CatBoost also incorporates novel techniques such as gradient-based adjustments and symmetric trees to improve performance.\n",
    "\n",
    "LightGBM: LightGBM is a gradient boosting framework designed for efficient training and prediction speed. It uses techniques like Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to reduce the number of data instances and optimize the training process. LightGBM is particularly suitable for large-scale datasets and has gained popularity for its speed and performance.\n",
    "\n",
    "These are just a few examples of boosting algorithms, and there are other variants and custom implementations available as well. The choice of boosting algorithm depends on factors such as the nature of the problem, dataset size, computational resources, and specific requirements of the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa4c28-7a48-4b32-ad39-8adf4fe2163b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b84ea34-3948-4f3c-a1b0-b27612e34867",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f782b0c-58be-4caf-8fd2-9c090ca43240",
   "metadata": {},
   "source": [
    "ans - Boosting algorithms have various parameters that can be tuned to optimize performance and control the behavior of the ensemble model. Here are some common parameters found in boosting algorithms:\n",
    "\n",
    "Number of Iterations: Boosting algorithms typically run for a fixed number of iterations or until a stopping criterion is met. This parameter determines the maximum number of weak learners that will be trained and added to the ensemble.\n",
    "\n",
    "Learning Rate (or Shrinkage): The learning rate controls the contribution of each weak learner to the final ensemble. It scales the weight of each weak learner's prediction before combining them. A lower learning rate can help prevent overfitting but may require more iterations for convergence.\n",
    "\n",
    "Weak Learner Parameters: Boosting algorithms use weak learners, such as decision trees or rules, as the base models. The parameters of these weak learners, such as the maximum tree depth, minimum number of samples per leaf, or rule complexity, can be adjusted to control their individual behavior and complexity.\n",
    "\n",
    "Loss Function: The loss function defines the measure of error or discrepancy that the boosting algorithm aims to minimize during training. Different boosting algorithms support various loss functions, such as squared loss for regression problems or exponential loss for binary classification.\n",
    "\n",
    "Regularization Parameters: Regularization is used to control the complexity of the ensemble model and prevent overfitting. Parameters such as regularization strength, L1 or L2 regularization, or maximum feature or tree depth can be adjusted to impose constraints on the weak learners or the overall model.\n",
    "\n",
    "Subsampling Parameters: Some boosting algorithms, like stochastic gradient boosting, support subsampling techniques to reduce overfitting and enhance training efficiency. Parameters such as subsample ratio or feature subsetting ratio determine the portion of the training data or features used in each iteration.\n",
    "\n",
    "Feature Importance Parameters: Boosting algorithms often provide feature importance measures, indicating the relative importance of each feature in the ensemble model. Parameters related to feature importance, such as feature selection thresholds or methods for calculating importance scores, can be adjusted to control feature selection or pruning.\n",
    "\n",
    "It's important to note that the specific parameters and their names may vary depending on the boosting algorithm or the software/library being used. Parameter tuning is crucial for optimizing the performance of the boosting model, and it often involves a combination of manual tuning, grid search, or other hyperparameter optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193fb2b-8740-4652-95cd-e10f1dbad626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edd1ac0-a0bd-41e5-9a14-67bd0ff61502",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499302d-a953-4855-809a-f7ba81a4d4c6",
   "metadata": {},
   "source": [
    "ans - Boosting algorithms combine weak learners in a sequential manner to create a strong learner, which is the final ensemble model. The process of combining weak learners varies depending on the specific boosting algorithm used, but here is a general overview of how boosting algorithms accomplish this:\n",
    "\n",
    "Initialization: The boosting algorithm starts by initializing the ensemble with a weak learner, typically a simple model such as a decision tree or a rule. This weak learner is trained on the initial training data.\n",
    "\n",
    "Weighted Training and Prediction: After the initialization, the boosting algorithm adjusts the weights of the training instances based on their classification errors or residuals. The subsequent weak learners are trained on modified versions of the training data, where the weights emphasize the misclassified instances or the instances with large residuals.\n",
    "\n",
    "Weighted Combination: As each weak learner is trained, their predictions are combined with the predictions of the previously trained weak learners. The specific combination method depends on the boosting algorithm. For example, AdaBoost calculates the weighted majority vote of the weak learners, where the weights are determined based on their performance. Gradient Boosting algorithms perform an additive combination by summing the predictions of the weak learners, each weighted by a learning rate.\n",
    "\n",
    "Sequential Training: The boosting algorithm continues the training process by iteratively training new weak learners, each one focusing on the instances that were misclassified or had large residuals by the ensemble of weak learners trained so far. The weights of the instances and the weak learners are updated accordingly.\n",
    "\n",
    "Stopping Criterion: The boosting process continues until a stopping criterion is met, such as reaching a specified number of iterations, achieving a certain level of performance, or when further iterations no longer improve the model's accuracy.\n",
    "\n",
    "Final Ensemble Prediction: Once the boosting algorithm has trained the desired number of weak learners, the final ensemble prediction is made by aggregating the predictions of all weak learners. The specific method of aggregation depends on the boosting algorithm. For example, it could be a weighted majority vote, a weighted average, or a weighted sum of the weak learners' predictions.\n",
    "\n",
    "By combining the predictions of multiple weak learners, boosting algorithms create a strong learner that leverages the individual strengths of each weak learner. The boosting process focuses on the difficult instances and adjusts the weights of the weak learners to emphasize their importance, leading to an ensemble model with improved predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1e5eb-a291-4f98-995c-efd58dd726c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6860f844-263e-4ae0-b6df-49e6ebd841b3",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e5467-6e84-4ae1-b09f-78e42dfad341",
   "metadata": {},
   "source": [
    "ans  - AdaBoost, short for Adaptive Boosting, is one of the most well-known boosting algorithms used for classification tasks. It was introduced by Freund and Schapire in 1996. AdaBoost works by iteratively training a sequence of weak learners and combining their predictions to create a strong ensemble model. Here's how the AdaBoost algorithm works:\n",
    "\n",
    "Initialization: At the beginning, each instance in the training set is assigned an equal weight. These weights indicate the importance of each instance during the training process.\n",
    "\n",
    "Iterative Training:\n",
    "a. Weak Learner Training: AdaBoost starts by training a weak learner on the weighted training data. A weak learner can be any classification algorithm that performs better than random guessing, such as a decision stump (a decision tree with a single split).\n",
    "b. Weighted Error Calculation: The weak learner's performance is evaluated by calculating its weighted classification error. The weighted error considers both the accuracy of the weak learner's predictions and the importance of each instance based on their weights.\n",
    "c. Weak Learner Weight Calculation: The weight of the weak learner is calculated based on its performance. A smaller weighted error leads to a higher weight, indicating that the weak learner's predictions are more reliable.\n",
    "d. Instance Weight Update: The weights of the misclassified instances are increased, amplifying their importance for the next iteration. This adjustment ensures that subsequent weak learners focus more on the difficult instances that were not well classified by the previous learners.\n",
    "\n",
    "Ensemble Combination:\n",
    "a. Weak Learner Contribution: Each weak learner's contribution to the ensemble model is determined by its weight, which is proportional to its performance. The better a weak learner performs, the more influence it has on the final predictions.\n",
    "b. Ensemble Prediction: To make predictions, AdaBoost combines the predictions of all weak learners using a weighted majority vote. The weight of each weak learner determines the contribution of its predictions to the final ensemble.\n",
    "\n",
    "Iteration Termination: The iterative process continues for a predefined number of iterations or until a specified criterion is met, such as reaching a maximum number of weak learners or achieving a desired level of performance.\n",
    "\n",
    "Final Ensemble Prediction: Once the boosting iterations are complete, the final ensemble model is formed by aggregating the weighted predictions of all weak learners. The final prediction is typically obtained by applying a threshold to the sum of the weighted predictions.\n",
    "\n",
    "AdaBoost's iterative training process focuses on instances that are difficult to classify correctly, allowing subsequent weak learners to improve the overall performance. By combining the predictions of multiple weak learners, AdaBoost creates a strong ensemble model that can make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e7a76-d8a6-4e4e-90bd-92ef2b3648dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5195c9b5-4652-4bdc-83ea-0751fdc8fe4b",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21961693-e2d5-4cae-8b7c-160687ab39a9",
   "metadata": {},
   "source": [
    "ans - The AdaBoost algorithm uses an exponential loss function as the default choice for classification tasks. The exponential loss function is a convex function that is well-suited for boosting algorithms. It encourages the boosting algorithm to focus on instances that are misclassified by the weak learners.\n",
    "\n",
    "The exponential loss function is defined as:\n",
    "\n",
    "L(y, f(x)) = exp(-y * f(x))\n",
    "\n",
    "where:\n",
    "\n",
    "L(y, f(x)) is the exponential loss between the true class label y and the predicted class score f(x),\n",
    "y is the true class label, which takes the values of -1 or +1 for binary classification,\n",
    "f(x) is the predicted class score or output of the ensemble model.\n",
    "In AdaBoost, the weak learners are trained to minimize the weighted exponential loss function. During each iteration, the weak learner aims to find the best split or rule that minimizes the weighted exponential loss by adjusting its parameters or thresholds.\n",
    "\n",
    "The weights assigned to the training instances in AdaBoost are updated based on the exponential loss. The misclassified instances receive higher weights, making them more important for subsequent iterations. As a result, AdaBoost focuses on improving the classification of difficult instances in subsequent iterations.\n",
    "\n",
    "It's important to note that the choice of loss function can be modified in boosting algorithms, depending on the specific requirements of the problem. However, the exponential loss function is the default choice in AdaBoost due to its properties and effectiveness in handling misclassified instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d40d1-dcc2-4284-a895-0240127aa652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0886c57e-a983-4034-aed1-e039027039ff",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e12413-0040-47d4-b5f3-b2d536fe1f2d",
   "metadata": {},
   "source": [
    "ans - In the AdaBoost algorithm, the weights of misclassified samples are updated to emphasize their importance in subsequent iterations. The weight update process in AdaBoost can be summarized in the following steps:\n",
    "\n",
    "Initialization: Initially, each sample in the training set is assigned an equal weight, which is typically set to 1/N, where N is the total number of samples in the training set.\n",
    "\n",
    "Weak Learner Training: AdaBoost trains a weak learner, such as a decision stump, on the weighted training data. The weak learner's objective is to minimize the weighted error, where the weights of the training samples indicate their importance.\n",
    "\n",
    "Weighted Error Calculation: After the weak learner is trained, its performance is evaluated by calculating the weighted error. The weighted error takes into account both the accuracy of the weak learner's predictions and the importance of each sample based on their weights.\n",
    "\n",
    "Weight Update:\n",
    "a. Weighted Error Contribution: The weighted error contribution of the weak learner is calculated as the sum of the weights of the misclassified samples.\n",
    "b. Weak Learner Weight Calculation: The weight of the weak learner is calculated based on its weighted error contribution. A smaller weighted error contribution leads to a higher weight for the weak learner, indicating that its predictions are more reliable.\n",
    "c. Misclassified Sample Weight Update: The weights of the misclassified samples are increased. The weight update is typically done using the formula:\n",
    "\n",
    "w_i = w_i * exp(alpha)\n",
    "\n",
    "where w_i is the weight of the i-th sample, alpha is the weight assigned to the weak learner, and exp() is the exponential function.\n",
    "\n",
    "Increasing the weights of the misclassified samples makes them more influential in the subsequent training iterations, allowing the weak learners to focus on these difficult instances.\n",
    "\n",
    "Weight Normalization: After updating the weights of the misclassified samples, the weights of all samples are normalized to ensure that they sum up to 1. This normalization step helps maintain the overall weight distribution and prevent the weights from growing too large or too small.\n",
    "\n",
    "By updating the weights of the misclassified samples, AdaBoost puts more emphasis on the instances that are difficult to classify correctly. This iterative weight update process allows subsequent weak learners to focus on these difficult instances, leading to an ensemble model that performs well on challenging samples.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783fdec-fb7e-4a6a-a440-1a62236914d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c29b752c-c873-4b64-80bd-356e100b96f7",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385c665-506e-42d1-a802-85a784f4ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
