{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b422374-a044-4ab4-b1c5-a9f9319f5efa",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7c716-e62b-471a-a118-9a4841dfcc8e",
   "metadata": {},
   "source": [
    "In linear regression models, the concept of R-squared (R²) is used to evaluate the goodness of fit of the model to the observed data. R-squared represents the proportion of the variance in the dependent variable (target variable) that can be explained by the independent variables (predictor variables) in the model.\n",
    "\n",
    "To calculate R-squared, you first need to fit the linear regression model to your data. Once the model is fitted, you calculate the sum of squared errors (SSE), which represents the sum of the squared differences between the actual values of the dependent variable and the predicted values given by the linear regression model.\n",
    "\n",
    "Next, you calculate the total sum of squares (SST), which represents the sum of the squared differences between the actual values of the dependent variable and the mean value of the dependent variable.\n",
    "\n",
    "Finally, R-squared is calculated as:\n",
    "\n",
    "R² = 1 - (SSE / SST)\n",
    "\n",
    "R-squared ranges between 0 and 1. A value of 0 indicates that the independent variables have no explanatory power in predicting the dependent variable, while a value of 1 indicates that the independent variables perfectly explain the dependent variable.\n",
    "\n",
    "Interpreting R-squared can be subjective and context-dependent. Generally, a higher R-squared value suggests that a larger proportion of the variance in the dependent variable is explained by the independent variables. However, R-squared alone doesn't indicate whether the model is adequate or not, as it doesn't consider other important factors such as the sample size, the appropriateness of the model assumptions, or the presence of omitted variables. It is always recommended to consider multiple evaluation metrics and assess the model's performance in conjunction with domain knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058dbef-befe-457c-a24b-e7df69bbee80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d31a8fd2-782c-48e3-9641-78a11c45526b",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a893e4-cf3d-4f0b-8afa-7660d4f05da7",
   "metadata": {},
   "source": [
    "ans - Adjusted R-squared is a modified version of R-squared that takes into account the number of predictor variables in the linear regression model. While R-squared provides an indication of the goodness of fit, it can be biased and misleading when additional predictors are added to the model.\n",
    "\n",
    "The adjusted R-squared adjusts for the number of predictor variables and penalizes the addition of irrelevant variables that do not contribute significantly to the model's explanatory power. It helps address the issue of overfitting, which occurs when a model performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "The formula to calculate adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "where:\n",
    "\n",
    "R² is the regular R-squared\n",
    "n is the sample size (number of observations)\n",
    "p is the number of predictor variables (excluding the intercept term)\n",
    "The adjusted R-squared ranges from negative infinity to 1. A higher adjusted R-squared indicates that the independent variables have a stronger explanatory power while considering the complexity of the model. It provides a more conservative estimate of the model's goodness of fit by accounting for the number of predictors.\n",
    "\n",
    "The adjusted R-squared is often preferred over the regular R-squared when comparing models with a different number of predictors. It helps in selecting the model that strikes a balance between explanatory power and complexity. However, like R-squared, it should not be solely relied upon for model evaluation, and other metrics and considerations should also be taken into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4e628-0160-4de1-ae3e-2503046cb233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f9bbe8-9205-4426-9935-d7c0f863ca86",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a48901-5ac9-419a-bb2a-ed1c9d0d78b2",
   "metadata": {},
   "source": [
    "ans - Adjusted R-squared is more appropriate to use when comparing and evaluating models with a different number of predictor variables. It addresses the issue of overfitting by penalizing the addition of irrelevant variables that do not contribute significantly to the model's explanatory power. Here are some situations when adjusted R-squared is particularly useful:\n",
    "\n",
    "Model comparison: When comparing multiple regression models with different numbers of predictor variables, adjusted R-squared helps in selecting the model that strikes a balance between goodness of fit and model complexity. It allows for a fair comparison by considering the trade-off between explanatory power and the degrees of freedom used by the predictors.\n",
    "\n",
    "Variable selection: Adjusted R-squared can assist in the variable selection process. It helps identify the most relevant and informative variables by giving higher scores to models that explain more of the variation in the dependent variable while considering the number of predictors. Models with higher adjusted R-squared values are generally preferred, as they provide better explanatory power with a parsimonious set of variables.\n",
    "\n",
    "Model simplicity: Adjusted R-squared encourages simplicity in the model by penalizing the inclusion of unnecessary predictors. It helps prevent overfitting, which occurs when a model is too complex and fits the noise in the data rather than the underlying patterns. By considering the model's complexity, adjusted R-squared provides a more conservative estimate of the model's goodness of fit.\n",
    "\n",
    "However, it's important to note that adjusted R-squared should not be the sole criterion for model evaluation. It is just one of several metrics that should be considered, along with other factors such as the model's assumptions, the interpretability of the variables, and the overall fit of the model. Additionally, domain knowledge and subject matter expertise should always be taken into account when interpreting and selecting models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b27594-7fd3-4b28-b9ff-c8528acf672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d834c64-fde8-474a-a6fa-183b204ba605",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55b42d-4b2d-4065-9c3d-e150d3f07d07",
   "metadata": {},
   "source": [
    "ans - RMSE (Root Mean Square Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics in regression analysis to measure the performance and accuracy of a regression model. These metrics quantify the difference between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "Root Mean Square Error (RMSE):\n",
    "RMSE is a popular metric that represents the square root of the average of the squared differences between the predicted and actual values. It provides a measure of the typical or average magnitude of the errors.\n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "To calculate RMSE, follow these steps:\n",
    "\n",
    "Compute the squared difference between each predicted value and its corresponding actual value.\n",
    "Calculate the mean of the squared differences.\n",
    "Take the square root of the mean to obtain the RMSE.\n",
    "Mean Squared Error (MSE):\n",
    "MSE is a metric that represents the average of the squared differences between the predicted and actual values. Squaring the errors eliminates the negative signs and emphasizes larger errors.\n",
    "To calculate MSE, follow these steps:\n",
    "\n",
    "Compute the squared difference between each predicted value and its corresponding actual value.\n",
    "Calculate the mean of the squared differences.\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is a metric that represents the average of the absolute differences between the predicted and actual values. It provides a measure of the average magnitude of the errors without considering their direction.\n",
    "To calculate MAE, follow these steps:\n",
    "\n",
    "Compute the absolute difference between each predicted value and its corresponding actual value.\n",
    "Calculate the mean of the absolute differences.\n",
    "Interpretation:\n",
    "\n",
    "RMSE: It is a measure of the typical or average magnitude of the errors. A lower RMSE indicates better predictive accuracy, with a value of 0 representing a perfect fit.\n",
    "\n",
    "MSE: It is similar to RMSE but without taking the square root. MSE gives higher weight to larger errors. It is useful for penalizing large errors in optimization or model training.\n",
    "\n",
    "MAE: It represents the average magnitude of the errors. MAE is easier to interpret as it is on the same scale as the dependent variable. A lower MAE indicates better accuracy, with a value of 0 representing a perfect fit.\n",
    "\n",
    "It's important to note that the choice of which metric to use depends on the specific context and the importance given to different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee65359-0b55-4c32-b602-41c59eba8057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c25d17-d75c-49c9-934e-42ec9103787f",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cbb9f-3f20-4c58-97f3-0ee490d992d4",
   "metadata": {},
   "source": [
    "ans - RMSE, MSE, and MAE are widely used evaluation metrics in regression analysis, and each has its own advantages and disadvantages. Let's discuss them:\n",
    "\n",
    "Advantages of RMSE:\n",
    "\n",
    "Sensitivity to large errors: RMSE is sensitive to larger errors due to the squared term in its calculation. This makes it useful when the impact of larger errors on the overall performance of the model needs to be emphasized.\n",
    "\n",
    "Differentiability: RMSE is a differentiable metric, which makes it suitable for optimization algorithms that rely on gradient-based methods.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "Magnitude bias: RMSE is affected by the scale of the dependent variable. This means that if the scale of the variable changes, the RMSE values will change as well, making it difficult to compare models with different scales. It is important to normalize or standardize the variables before comparing RMSE values.\n",
    "\n",
    "Lack of interpretability: RMSE doesn't have the same interpretation as the dependent variable, as it is measured in squared units. It may not be as intuitive for non-technical audiences.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "Mathematical properties: MSE is a mathematically convenient metric due to its differentiability, which makes it suitable for optimization algorithms.\n",
    "\n",
    "Emphasis on large errors: Like RMSE, MSE gives more weight to larger errors due to the squared term in its calculation.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "Similar to RMSE, MSE is influenced by the scale of the dependent variable. It may not be directly comparable across models with different scales unless the variables are normalized.\n",
    "\n",
    "Lack of interpretability: MSE suffers from the same lack of interpretability as RMSE, as it is measured in squared units.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "Scale invariance: MAE is not affected by the scale of the dependent variable, making it a suitable metric for comparing models with different scales without the need for normalization.\n",
    "\n",
    "Interpretability: MAE has the same unit of measurement as the dependent variable, making it more intuitive and easier to interpret, especially for non-technical audiences.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "Insensitivity to larger errors: MAE treats all errors equally, without giving more weight to larger errors. This may not be desirable in situations where larger errors are more critical or impactful.\n",
    "\n",
    "Non-differentiability: Unlike RMSE and MSE, MAE is not differentiable at all points. This can be a disadvantage when using optimization algorithms that require differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05d3c6-3ced-4230-a0aa-4ee76e807b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8975e6b5-f3d2-415b-b0a3-b342d22cb660",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4261a-ade6-4c28-85b5-53fa10e9d492",
   "metadata": {},
   "source": [
    "ans - Lasso regularization, also known as L1 regularization, is a technique used in linear regression to add a penalty term to the objective function. It helps in reducing the complexity of the model and performing feature selection by encouraging sparsity, i.e., pushing the coefficients of irrelevant or less important features towards zero.\n",
    "\n",
    "The Lasso regularization technique adds the absolute values of the coefficients multiplied by a regularization parameter (λ) to the least squares objective function. The objective function of Lasso regression is:\n",
    "\n",
    "Minimize: SSE + λ * ∑|βi|\n",
    "\n",
    "where:\n",
    "\n",
    "SSE is the sum of squared errors (same as in ordinary linear regression).\n",
    "∑|βi| represents the sum of the absolute values of the coefficients.\n",
    "λ is the regularization parameter that controls the strength of regularization. A higher λ leads to more coefficients being pushed towards zero.\n",
    "Differences between Lasso and Ridge regularization:\n",
    "\n",
    "Penalty term: Lasso uses the absolute values of the coefficients (L1 penalty) in the regularization term, while Ridge regularization (L2 regularization) uses the squared values of the coefficients.\n",
    "\n",
    "Sparsity: Lasso tends to produce sparse solutions by driving the coefficients of irrelevant features exactly to zero. This property makes Lasso useful for feature selection, as it can identify the most important predictors. In contrast, Ridge regularization does not lead to exact zero coefficients and maintains all the features, but with smaller magnitudes.\n",
    "\n",
    "Variable selection: Lasso performs automatic variable selection by setting some coefficients to zero. It selects a subset of the most relevant features, which can be beneficial in scenarios where there are many predictors and only a few are expected to have a significant impact. Ridge regularization, on the other hand, includes all the features in the model, albeit with smaller coefficients.\n",
    "\n",
    "When to use Lasso regularization:\n",
    "Lasso regularization is more appropriate to use when:\n",
    "\n",
    "Feature selection is desired, and it is important to identify the most relevant predictors.\n",
    "The data has a large number of features, and it is expected that only a few are truly important.\n",
    "Interpretability of the model is important, as Lasso produces sparse solutions with fewer nonzero coefficients.\n",
    "It's important to note that the choice between Lasso and Ridge regularization depends on the specific problem and the characteristics of the data. Cross-validation techniques can help in determining the optimal value of the regularization parameter (λ) for both Lasso and Ridge regularization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c22e6-34ab-466c-aaee-af73b4022849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b61298fd-e557-4c88-870e-404d1052b4df",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588e0e4-cb28-4614-9f85-71e237daca01",
   "metadata": {},
   "source": [
    "ans - Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the objective function during model training. The penalty term discourages complex or high-variance models by constraining the magnitude of the coefficients, thus reducing the model's tendency to fit the noise in the training data.\n",
    "\n",
    "Let's consider an example to illustrate this:\n",
    "\n",
    "Suppose we have a dataset with two predictor variables, x1 and x2, and a continuous target variable y. We want to build a linear regression model to predict y based on x1 and x2. However, our dataset has a limited number of observations, and we want to prevent overfitting.\n",
    "\n",
    "Without regularization:\n",
    "In ordinary linear regression, the model aims to minimize the sum of squared errors (SSE) between the predicted and actual values. Without regularization, the model can freely increase the magnitude of the coefficients to fit the noise in the training data. This can lead to overfitting, where the model becomes too complex and fails to generalize well to new, unseen data.\n",
    "\n",
    "With regularization:\n",
    "To prevent overfitting, we can use a regularized linear model such as Ridge or Lasso regression.\n",
    "\n",
    "Ridge Regression: Ridge regression adds a penalty term to the objective function, which is the sum of squared coefficients multiplied by a regularization parameter (λ). This penalty term encourages smaller coefficient values, effectively shrinking the coefficients towards zero. As a result, Ridge regression reduces the impact of less important variables and prevents overfitting.\n",
    "\n",
    "Lasso Regression: Lasso regression also adds a penalty term to the objective function, but it uses the sum of the absolute values of the coefficients multiplied by a regularization parameter (λ). Lasso regression not only shrinks the coefficients but also has the property of exact zero coefficients. It performs feature selection by pushing the coefficients of irrelevant features to exactly zero.\n",
    "\n",
    "In both cases, the regularization term controls the amount of shrinkage applied to the coefficients. By introducing a penalty for large coefficients, regularized linear models find a balance between minimizing the SSE and reducing model complexity. This helps prevent overfitting and improves the model's generalization ability to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eda277-0ee0-4b94-bac0-787a0b1e95f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd062cc3-0b13-41ab-805e-ef04ae685a29",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc308625-0705-4113-8987-bd787cfd976f",
   "metadata": {},
   "source": [
    "ans  - While regularized linear models, such as Ridge and Lasso regression, are effective techniques for regression analysis, they have certain limitations and may not always be the best choice in all scenarios. Let's discuss some of these limitations:\n",
    "\n",
    "Linearity assumption: Regularized linear models assume a linear relationship between the predictors and the target variable. If the relationship is highly nonlinear, regularized linear models may not capture the underlying patterns effectively. In such cases, more flexible models like decision trees, support vector machines, or neural networks may be more appropriate.\n",
    "\n",
    "Interpretability: Regularized linear models can be less interpretable compared to simple linear regression. The regularization process may shrink coefficients towards zero or eliminate them entirely, making it challenging to directly interpret the impact of each predictor on the target variable. If interpretability is crucial, a simpler linear regression model without regularization may be preferred.\n",
    "\n",
    "Feature selection limitations: While Lasso regression performs feature selection by driving some coefficients to exactly zero, Ridge regression does not eliminate features completely. If the dataset contains a large number of highly correlated predictors, Ridge regression may not effectively select the most important features. In such cases, feature engineering or other feature selection techniques may be required.\n",
    "\n",
    "Sensitivity to hyperparameters: Regularized linear models have hyperparameters that need to be tuned, such as the regularization parameter (λ). The performance of these models can be sensitive to the choice of hyperparameters, and selecting the optimal values may require cross-validation or grid search. If the dataset is small or the relationship between predictors and the target variable is complex, finding the right hyperparameters can be challenging.\n",
    "\n",
    "Outliers: Regularized linear models may not handle outliers well. Outliers can have a disproportionate influence on the coefficients, even with regularization. If the dataset contains influential outliers, other robust regression techniques or outlier detection methods may be more suitable.\n",
    "\n",
    "Non-linear interactions: Regularized linear models assume linear relationships between predictors and the target variable. If the relationship involves non-linear interactions or higher-order terms, regularized linear models may not capture these complexities effectively. Non-linear regression models or polynomial regression can be considered in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6148617-c6be-4d48-b205-17a965dc5e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6486f16c-a6d3-4589-8855-bd0cec8de3fa",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350662ae-58a1-46fd-8df5-9cfbe5985796",
   "metadata": {},
   "source": [
    "ans  - To determine which model is the better performer between Model A and Model B, we need to consider the evaluation metrics provided (RMSE of 10 for Model A and MAE of 8 for Model B).\n",
    "\n",
    "In this case, since both models have different evaluation metrics (RMSE and MAE), we should consider the specific context and priorities of the problem to make a decision.\n",
    "\n",
    "If we prioritize the reduction of larger errors and want to penalize the influence of outliers more, RMSE would be a suitable metric. In that case, a lower RMSE indicates better performance. Therefore, Model A with an RMSE of 10 would be considered the better performer compared to Model B with an MAE of 8.\n",
    "\n",
    "However, if the emphasis is on the average magnitude of errors without considering their direction, MAE would be a better metric to use. In that case, a lower MAE indicates better performance. Based on MAE alone, Model B with an MAE of 8 would be considered the better performer compared to Model A with an RMSE of 10.\n",
    "\n",
    "It's important to note that the choice of metric depends on the specific context, priorities, and the problem at hand. Both RMSE and MAE have their own advantages and limitations. RMSE gives more weight to larger errors due to the squared term, while MAE treats all errors equally.\n",
    "\n",
    "Limitations of the metric choice:\n",
    "The limitations of the chosen metric should also be considered. For example, RMSE and MAE may not be directly comparable since they are measured in different units. Additionally, both metrics may not provide a complete understanding of the model's performance. It is essential to consider other evaluation metrics, perform cross-validation, and take into account the specific requirements and constraints of the problem to make an informed decision about the better-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572a1f5-a29f-4e92-8122-4d0e1ace72a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a136bee6-ef87-489f-9097-79bc7a7dc194",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f6aa2-856a-41ac-aa75-f49f908f77e7",
   "metadata": {},
   "source": [
    "ans - To determine which regularized linear model is the better performer between Model A (Ridge regularization with λ = 0.1) and Model B (Lasso regularization with λ = 0.5), we need to consider the specific context and objectives of the problem. The choice will depend on the priorities and trade-offs associated with each regularization method.\n",
    "\n",
    "Ridge regularization:\n",
    "Ridge regularization adds a penalty term to the objective function that is proportional to the sum of squared coefficients multiplied by a regularization parameter (λ). Ridge regularization aims to reduce the impact of less important features while keeping all features in the model.\n",
    "\n",
    "Lasso regularization:\n",
    "Lasso regularization also adds a penalty term to the objective function, but it is proportional to the sum of the absolute values of the coefficients multiplied by a regularization parameter (λ). Lasso regularization encourages sparsity and feature selection by driving some coefficients exactly to zero. It selects a subset of the most relevant predictors.\n",
    "\n",
    "Considering the provided information, we cannot directly compare the performance of Model A and Model B based solely on the regularization parameter values (λ). The choice of the better performer depends on the specific problem and the trade-offs associated with each regularization method.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "\n",
    "Ridge regularization: Ridge regularization does not perform feature selection by driving coefficients exactly to zero. It may retain all features in the model, albeit with smaller magnitudes. This can be advantageous when all predictors are expected to contribute to the target variable, or when multicollinearity is present. Ridge regularization tends to be less sensitive to the specific choice of λ.\n",
    "\n",
    "Lasso regularization: Lasso regularization performs feature selection by driving some coefficients to exactly zero. It selects a subset of the most important predictors, effectively performing automatic variable selection. Lasso regularization can be advantageous when feature sparsity is expected, and only a few predictors are assumed to have a significant impact. However, Lasso regularization may not handle highly correlated features well, and the selection of the optimal λ can be more critical.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
