{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa519388-8cd8-4cc2-9196-dde5e9a30871",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03846e02-01d7-46c8-9f8d-761b2f955938",
   "metadata": {},
   "source": [
    "ans -  \n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale and transform the features of a dataset so that they fall within a specific range. The goal is to bring all the features to a common scale without distorting the original distribution of the data.\n",
    "\n",
    "The formula for Min-Max scaling is given by:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "X is the original value of a feature, \n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of that feature in the dataset, and \n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of that feature in the dataset.\n",
    "\n",
    "Here's how Min-Max scaling is applied in data preprocessing:\n",
    "\n",
    "Identify the minimum (\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    " ) and maximum (\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    " ) values for each feature in the dataset.\n",
    "Apply the Min-Max scaling formula to transform each feature.\n",
    "This technique is particularly useful when working with algorithms that are sensitive to the scale of input features, such as gradient-based optimization algorithms used in machine learning models.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with a feature \"Age\" ranging from 20 to 60. You want to apply Min-Max scaling to bring the values of this feature within the range [0, 1]. The minimum value (\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    " ) is 20, and the maximum value (\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    " ) is 60.\n",
    "\n",
    "Let's say you have a data point with \n",
    "�\n",
    "=\n",
    "30\n",
    "X=30. Applying the Min-Max scaling formula:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "30\n",
    "−\n",
    "20\n",
    "60\n",
    "−\n",
    "20\n",
    "=\n",
    "10\n",
    "40\n",
    "=\n",
    "0.25\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "60−20\n",
    "30−20\n",
    "​\n",
    " = \n",
    "40\n",
    "10\n",
    "​\n",
    " =0.25\n",
    "\n",
    "So, the scaled value for \n",
    "�\n",
    "=\n",
    "30\n",
    "X=30 after Min-Max scaling is 0.25. This process is repeated for each data point in the \"Age\" feature, ensuring that all values are transformed to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878666f1-870d-4240-acef-53d31addc2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8552fd-d378-4a3a-bda3-423ea842bc35",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55247211-7819-431f-805e-9a59442f99a8",
   "metadata": {},
   "source": [
    "ans - The Unit Vector technique, also known as \"Unit Vector Scaling\" or \"Normalization,\" is another method for feature scaling similar to Min-Max scaling. The key difference lies in the normalization formula used and the specific range to which the values are scaled.\n",
    "\n",
    "In Unit Vector scaling, each feature vector is scaled so that it becomes a unit vector (a vector with a magnitude of 1). The formula for Unit Vector scaling is given by:\n",
    "\n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "∥\n",
    "�\n",
    "∥\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "∥X∥\n",
    "X\n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "X is the original feature vector, and \n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥X∥ denotes the Euclidean norm or magnitude of the vector.\n",
    "\n",
    "Here's how Unit Vector scaling differs from Min-Max scaling:\n",
    "\n",
    "Range: Min-Max scaling scales the values of each feature to a specific range (e.g., [0, 1]), while Unit Vector scaling scales the entire feature vector to have a magnitude of 1.\n",
    "\n",
    "Direction: Min-Max scaling preserves the relative distances between values in the original feature, but it may change the direction of the vector. Unit Vector scaling not only scales the values but also ensures that the direction of the vector remains the same.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with two features, \n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " , and you want to apply Unit Vector scaling. The original feature vector is \n",
    "[\n",
    "3\n",
    ",\n",
    "4\n",
    "]\n",
    "[3,4].\n",
    "\n",
    "Calculate the Euclidean norm (\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥X∥):\n",
    "∥\n",
    "�\n",
    "∥\n",
    "=\n",
    "3\n",
    "2\n",
    "+\n",
    "4\n",
    "2\n",
    "=\n",
    "9\n",
    "+\n",
    "16\n",
    "=\n",
    "25\n",
    "=\n",
    "5\n",
    "∥X∥= \n",
    "3 \n",
    "2\n",
    " +4 \n",
    "2\n",
    " \n",
    "​\n",
    " = \n",
    "9+16\n",
    "​\n",
    " = \n",
    "25\n",
    "​\n",
    " =5\n",
    "\n",
    "Apply Unit Vector scaling:\n",
    "�\n",
    "new\n",
    "=\n",
    "[\n",
    "3\n",
    ",\n",
    "4\n",
    "]\n",
    "5\n",
    "=\n",
    "[\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    "]\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "5\n",
    "[3,4]\n",
    "​\n",
    " =[ \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " ]\n",
    "\n",
    "So, the scaled unit vector is \n",
    "[\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    "]\n",
    "[ \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " ]. This ensures that the magnitude of the vector is 1, and the direction remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6839a-6d53-4d80-8612-41711d0c538f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07fa4ede-baf2-4318-be37-e279fdaa7670",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765109c-1924-427f-a626-225464d05d5d",
   "metadata": {},
   "source": [
    "ans - Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in the field of machine learning and statistics. Its primary goal is to transform high-dimensional data into a lower-dimensional representation, capturing the most important information while minimizing the loss of variance.\n",
    "\n",
    "The key idea behind PCA is to find the principal components, which are linear combinations of the original features, ordered by the amount of variance they explain. The first principal component explains the most variance, the second explains the second most, and so on. By selecting a subset of these components, one can represent the data in a lower-dimensional space.\n",
    "\n",
    "Here are the main steps involved in PCA:\n",
    "\n",
    "Standardization: Standardize the features to have zero mean and unit variance. This step ensures that all features are on a similar scale.\n",
    "\n",
    "Covariance Matrix: Calculate the covariance matrix of the standardized features. The covariance matrix describes the relationships between different features.\n",
    "\n",
    "Eigendecomposition: Perform eigendecomposition on the covariance matrix to obtain eigenvalues and corresponding eigenvectors. The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the magnitude of the variance in those directions.\n",
    "\n",
    "Select Principal Components: Sort the eigenvectors by their corresponding eigenvalues in decreasing order. The top \n",
    "�\n",
    "k eigenvectors (principal components) are chosen to form a new feature space, where \n",
    "�\n",
    "k is the desired dimensionality of the reduced data.\n",
    "\n",
    "Projection: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a simple example with a dataset containing two features, \n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " . We want to reduce the dimensionality to one dimension (use only the first principal component).\n",
    "\n",
    "Standardization:\n",
    "Standardize the features to have zero mean and unit variance.\n",
    "\n",
    "Covariance Matrix:\n",
    "Calculate the covariance matrix:\n",
    "\n",
    "Cov\n",
    "=\n",
    "[\n",
    "Var\n",
    "(\n",
    "�\n",
    "1\n",
    ")\n",
    "Cov\n",
    "(\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ")\n",
    "Cov\n",
    "(\n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "1\n",
    ")\n",
    "Var\n",
    "(\n",
    "�\n",
    "2\n",
    ")\n",
    "]\n",
    "Cov=[ \n",
    "Var(X \n",
    "1\n",
    "​\n",
    " )\n",
    "Cov(X \n",
    "2\n",
    "​\n",
    " ,X \n",
    "1\n",
    "​\n",
    " )\n",
    "​\n",
    "  \n",
    "Cov(X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " )\n",
    "Var(X \n",
    "2\n",
    "​\n",
    " )\n",
    "​\n",
    " ]\n",
    "\n",
    "Eigendecomposition:\n",
    "Perform eigendecomposition to obtain eigenvalues (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    "λ \n",
    "1\n",
    "​\n",
    " ,λ \n",
    "2\n",
    "​\n",
    " ) and eigenvectors (\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    "v \n",
    "1\n",
    "​\n",
    " ,v \n",
    "2\n",
    "​\n",
    " ).\n",
    "\n",
    "Select Principal Components:\n",
    "Select the eigenvector corresponding to the highest eigenvalue. In this case, let's say it is \n",
    "�\n",
    "1\n",
    "v \n",
    "1\n",
    "​\n",
    " .\n",
    "\n",
    "Projection:\n",
    "Project the original data onto the first principal component:\n",
    "\n",
    "Reduced Dimension\n",
    "=\n",
    "�\n",
    "⋅\n",
    "�\n",
    "1\n",
    "Reduced Dimension=X⋅v \n",
    "1\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ab802-2607-4eaf-9e14-80c516dacffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5035c171-8783-414a-b6c5-f2c09c790f47",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a9f9d-07ca-4bb5-a95c-7c640a00163e",
   "metadata": {},
   "source": [
    "ans - Principal Component Analysis (PCA) and Feature Extraction are closely related concepts in the field of machine learning and data analysis. PCA is a technique used for dimensionality reduction, and it can be employed as a method for feature extraction.\n",
    "\n",
    "Relationship between PCA and Feature Extraction:\n",
    "\n",
    "Feature extraction involves transforming the original set of features into a new set of features, typically with a lower dimensionality, while preserving the most important information in the data. PCA is a specific method for feature extraction that achieves dimensionality reduction by projecting the original features onto a new set of orthogonal axes called principal components. These principal components are ordered by their importance, and the idea is to retain the most significant ones while discarding less relevant information.\n",
    "\n",
    "How PCA can be used for Feature Extraction:\n",
    "\n",
    "Covariance Matrix Calculation:\n",
    "\n",
    "Begin by calculating the covariance matrix of the original feature matrix.\n",
    "Eigendecomposition:\n",
    "\n",
    "Perform eigendecomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "Selection of Principal Components:\n",
    "\n",
    "Sort the eigenvalues in descending order. The corresponding eigenvectors represent the principal components. Choose the top k eigenvectors to retain the most important information, where k is the desired dimensionality of the reduced feature space.\n",
    "Projection:\n",
    "\n",
    "Project the original data onto the selected principal components to obtain the reduced feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe028f-132e-4047-817a-fad00780911d",
   "metadata": {},
   "source": [
    "Example - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d4846-48e2-4ce0-a951-9c5cbff9f43e",
   "metadata": {},
   "source": [
    "#Original Feature Matrix:\n",
    "\n",
    "| Height | Weight |\n",
    "|--------|--------|\n",
    "|  170   |   65   |\n",
    "|  160   |   55   |\n",
    "|  175   |   70   |\n",
    "|  162   |   58   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8a67a-4261-4b0c-a500-565ac26f1ea1",
   "metadata": {},
   "source": [
    "Covariance Matrix:\n",
    "    \n",
    "    | 100   20 |\n",
    "|  20   15 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef54e33-3f7f-48bf-bbf1-60c7cd6b45e8",
   "metadata": {},
   "source": [
    "Eigendecomposition:\n",
    "\n",
    "The eigenvalues are 105 and 10. The corresponding eigenvectors are [0.98, 0.20] and [-0.20, 0.98].\n",
    "\n",
    "\n",
    "Selection of Principal Components:\n",
    "\n",
    "Choose the top eigenvector, [0.98, 0.20], as the principal component.\n",
    "Projection:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2966cce-b091-47c3-b55d-871513992e4a",
   "metadata": {},
   "source": [
    "| Height | Weight | PCA1   |\n",
    "|--------|--------|--------|\n",
    "|  170   |   65   | 172.6  |\n",
    "|  160   |   55   | 161.2  |\n",
    "|  175   |   70   | 175.9  |\n",
    "|  162   |   58   | 162.1  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4818aa-c849-417b-851c-8ddcce0efb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e952d59-e57f-4cb4-a9a0-7a5ed69a76c8",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74acd647-55ce-4676-a4d0-d9fbb72dc72d",
   "metadata": {},
   "source": [
    "ans - Min-Max scaling is a common technique used in data preprocessing to normalize the features of a dataset. It transforms the data in a way that scales the values between a specified range, usually [0, 1]. This is particularly useful when features have different scales, and it helps in improving the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9710cf9-1541-4480-b02e-7c9a1f5c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      price    rating  delivery_time\n",
      "0  0.000000  0.833333          0.250\n",
      "1  0.666667  0.000000          0.625\n",
      "2  0.333333  1.000000          0.000\n",
      "3  1.000000  0.277778          1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'price': [10, 20, 15, 25],\n",
    "        'rating': [4.5, 3.0, 4.8, 3.5],\n",
    "        'delivery_time': [30, 45, 20, 60]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# Extract the features to be scaled\n",
    "features_to_scale = ['price', 'rating', 'delivery_time']\n",
    "data_to_scale = df[features_to_scale]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "\n",
    "# Create a new DataFrame with the scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=features_to_scale)\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb31e04-031a-47ea-b298-efbbcfc5622e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2f9c0f0-94f6-43db-af9c-2ca611f7715b",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff2191-c3cf-4acb-82aa-8a4400787d2b",
   "metadata": {},
   "source": [
    "ans - Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. When dealing with a dataset containing many features, such as financial data and market trends in the context of predicting stock prices, PCA can be employed to reduce the dimensionality of the dataset while retaining the most significant information.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset for predicting stock prices:\n",
    "\n",
    "1. Understanding the Dataset:\n",
    "\n",
    "Begin by understanding the dataset, including the nature of the features, their correlation, and the overall structure of the data.\n",
    "2. Data Preprocessing:\n",
    "\n",
    "Handle any missing values or outliers in the dataset, as PCA is sensitive to them.\n",
    "3. Standardization:\n",
    "\n",
    "Standardize the features to ensure they have a mean of 0 and a standard deviation of 1. This step is crucial because PCA is based on the covariance matrix, and standardization ensures that all features contribute equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90a2943-9457-4377-81ca-283966b1305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Assuming 'X' is your feature matrix\n",
    "X = data\n",
    "\n",
    "target_data = np.array([1, 2, 3])\n",
    "\n",
    "# Assuming 'y' is your target variable\n",
    "y = target_data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c61ad-1e82-4cf7-a6b5-7d51984c07a4",
   "metadata": {},
   "source": [
    "4. Applying PCA:\n",
    "\n",
    "Use PCA to transform the standardized features into principal components. You can choose the number of components based on the explained variance you want to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f94938-598d-410d-be10-43c4e350888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Choose the number of components (e.g., n_components=3)\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36324f-d7f9-484d-b08f-d920c9f7287f",
   "metadata": {},
   "source": [
    "5. Explained Variance:\n",
    "\n",
    "Evaluate the explained variance to understand how much information each principal component retains. This information guides you in choosing the appropriate number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b56c819-06ad-41c3-82f4-d59714a9a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22386057-dfdc-423a-8dd8-42992d2b1f2c",
   "metadata": {},
   "source": [
    "6. Choosing the Number of Components:\n",
    "\n",
    "Based on the explained variance, choose the number of principal components that retain a sufficient amount of information for your prediction task.\n",
    "7. Model Building:\n",
    "\n",
    "Use the reduced-dimension dataset (X_pca) for building your stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4381589c-985c-4cce-af56-21562ae045d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c730c-9ee6-49b8-af0f-d3e6cc9685b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc2b9efa-916c-4d34-9a73-6fe8d7d075c2",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0be95145-1b7f-4278-ab57-8ef44f6b0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Values: [ 1  5 10 15 20]\n",
      "Scaled Values (-1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new minimum and maximum values\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Min-Max scaling formula\n",
    "scaled_data = (data - np.min(data)) / (np.max(data) - np.min(data)) * (new_max - new_min) + new_min\n",
    "\n",
    "# Display the scaled values\n",
    "print(\"Original Values:\", data)\n",
    "print(\"Scaled Values (-1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfa2a6-3864-4185-b2c0-9eccc0d5ac08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b8f910-565c-49ed-95a6-1a487dc7e8a1",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b4fc4cc-ba6e-4bbf-858f-13be4bda069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain for 95.0% variance: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' is your feature matrix with shape (number_of_samples, number_of_features)\n",
    "data = np.array([[170, 65, 25, 1, 120],\n",
    "                 [160, 55, 30, 0, 130],\n",
    "                 [175, 70, 35, 1, 110],\n",
    "                 [162, 58, 28, 0, 125]])\n",
    "\n",
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_standardized)\n",
    "\n",
    "# Explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find the number of principal components that explain a desired percentage of variance (e.g., 95%)\n",
    "desired_explained_variance = 0.95\n",
    "num_components = np.argmax(cumulative_explained_variance >= desired_explained_variance) + 1\n",
    "\n",
    "print(f\"Number of principal components to retain for {desired_explained_variance * 100}% variance: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a8589-d0b7-4221-966c-96213af37ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840f2c1-c4c0-4098-9280-3d72a6060d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a74fda-5d00-40de-8251-0d44308a2c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
