{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1300171-417a-4f4c-af35-fb487e7ef7a4",
   "metadata": {},
   "source": [
    "Q1. Explain the basic concept of clustering and give examples of applications where clustering is useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef6b12-b216-4e45-be09-4b322429af82",
   "metadata": {},
   "source": [
    "ans - Clustering is a technique used in machine learning and data analysis to group similar data points together based on their inherent characteristics or properties. The goal of clustering is to identify patterns, structures, or relationships within a dataset without any prior knowledge of the groups or categories present.\n",
    "\n",
    "The basic concept of clustering involves assigning data points to clusters such that points within the same cluster are more similar to each other than to those in other clusters. The similarity between data points is typically measured using distance metrics, such as Euclidean distance or cosine similarity. The clustering algorithm aims to minimize the intra-cluster distance and maximize the inter-cluster distance.\n",
    "\n",
    "Here are a few examples of applications where clustering is useful:\n",
    "\n",
    "Customer Segmentation: Clustering can be applied to customer data in marketing to identify distinct groups or segments of customers based on their purchasing behavior, demographics, or preferences. This information helps businesses tailor their marketing strategies and personalize their offerings for each customer segment.\n",
    "\n",
    "Image Segmentation: Clustering algorithms can be used in computer vision to segment images into meaningful regions or objects. By grouping similar pixels together, clustering can be used for tasks like object recognition, image compression, and image editing.\n",
    "\n",
    "Document Clustering: Clustering techniques are beneficial in text mining and natural language processing to cluster documents based on their content. This allows for organizing large document collections, topic modeling, sentiment analysis, and information retrieval.\n",
    "\n",
    "Anomaly Detection: Clustering can help identify outliers or anomalies in datasets. By clustering normal data points together, any data point that does not fit into any cluster can be considered an anomaly. This is useful in various domains, such as fraud detection, network intrusion detection, and manufacturing quality control.\n",
    "\n",
    "Social Network Analysis: Clustering algorithms can be applied to analyze social networks and identify communities or groups of individuals with similar interests or relationships. This information can be used for targeted advertising, recommender systems, and understanding social dynamics.\n",
    "\n",
    "Bioinformatics: Clustering is used to analyze genetic data and identify patterns or groupings of genes or proteins. It aids in understanding genetic similarities, identifying disease subtypes, and predicting drug responses.\n",
    "\n",
    "These are just a few examples, but clustering techniques can be applied to various fields where grouping or categorization of data is beneficial for analysis, decision-making, or understanding underlying structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c42d4-1a4c-4f47-98d5-407fbbd86ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7566dcdd-6133-4336-87df-60ff074c65c6",
   "metadata": {},
   "source": [
    "Q2. What is DBSCAN and how does it differ from other clustering algorithms such as k-means and\n",
    "hierarchical clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984d395-bab6-437f-978f-d978050e94e6",
   "metadata": {},
   "source": [
    "ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that can discover clusters of arbitrary shape within a dataset. It differs from other clustering algorithms like k-means and hierarchical clustering in several ways:\n",
    "\n",
    "Handling Arbitrary Shape Clusters: DBSCAN is capable of identifying clusters of different shapes and sizes. It does not assume that clusters have a spherical or convex shape, which is a limitation of algorithms like k-means. DBSCAN can find clusters with irregular boundaries or clusters embedded within other clusters.\n",
    "\n",
    "Density-Based Clustering: DBSCAN defines clusters based on density connectivity. It identifies dense regions in the data by considering the minimum number of data points (MinPts) within a specified distance (epsilon or Eps). Points within this neighborhood are considered as core points, and neighboring core points form a single cluster. Points that are within the neighborhood of a core point but do not have enough nearby points are classified as border points, while points that are not within any core point's neighborhood are treated as noise or outliers.\n",
    "\n",
    "No Prespecified Number of Clusters: Unlike k-means and hierarchical clustering, DBSCAN does not require specifying the number of clusters in advance. It automatically determines the number of clusters based on the density and connectivity of the data points.\n",
    "\n",
    "Robust to Outliers: DBSCAN is robust to noise and outliers as it treats them as individual points that do not belong to any cluster. Outliers do not significantly affect the clustering results, unlike k-means, which can be influenced by outliers.\n",
    "\n",
    "Hierarchical Nature: Hierarchical clustering produces a nested hierarchy of clusters, forming a dendrogram. In contrast, DBSCAN does not produce a hierarchical structure directly. However, it can be used to build a hierarchical clustering by modifying the distance threshold parameter (epsilon) and considering different levels of density connectivity.\n",
    "\n",
    "Parameter Sensitivity: DBSCAN has two important parameters: epsilon (Eps), which defines the radius of the neighborhood, and MinPts, which determines the minimum number of points required to form a core point. The choice of these parameters affects the resulting clusters, and selecting appropriate values can be challenging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4336f97-9cd3-4e68-b475-a9a83def95fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348186c2-5719-41e0-9d15-a8ab1684111a",
   "metadata": {},
   "source": [
    "Q3. How do you determine the optimal values for the epsilon and minimum points parameters in DBSCAN\n",
    "clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524397b1-3e8a-44ef-8c47-229379e82f5d",
   "metadata": {},
   "source": [
    "ans - Determining the optimal values for the epsilon (Eps) and minimum points (MinPts) parameters in DBSCAN clustering can be done through a combination of visual inspection, domain knowledge, and evaluating the quality of the resulting clusters. Here are some approaches to consider:\n",
    "\n",
    "Visual Inspection: One way to determine suitable parameter values is to visually analyze the resulting clusters for different combinations of Eps and MinPts. Plot the data points and their clusters, varying the parameter values and observing the cluster shapes, sizes, and the presence of outliers. Adjust the parameters until the clusters align with your expectations and domain knowledge.\n",
    "\n",
    "K-Distance Plot: The k-distance plot is a useful tool to analyze the density-based characteristics of the data. It plots the k-distance of each data point against its index when sorted in increasing order. The k-distance is the distance to the kth nearest neighbor. By observing the plot, you can identify a threshold value for Eps where the plot experiences a significant change. This change indicates a transition from points within clusters to points in low-density areas. This threshold can guide the selection of Eps.\n",
    "\n",
    "Reachability Distance Plot: The reachability distance plot is another visualization technique that can help determine Eps. It plots the reachability distance of each point against its index. The reachability distance is the maximum distance between a point and any core point that can be reached within Eps. The plot can reveal natural breaks, allowing you to choose a suitable threshold for Eps.\n",
    "\n",
    "Domain Knowledge: Consider your domain knowledge and the characteristics of the dataset. Understanding the underlying data and the expected cluster structures can guide you in selecting appropriate parameter values. If you have prior knowledge about the average cluster size or the expected density of the data, it can inform your choices.\n",
    "\n",
    "Evaluation Metrics: Depending on the task and available labeled data, you can evaluate the clustering results using external metrics like silhouette score or internal metrics like the Davies-Bouldin index or the Calinski-Harabasz index. Experiment with different parameter values and assess the quality of the resulting clusters using these metrics. Choose the parameter values that yield the best clustering performance according to the selected metric.\n",
    "\n",
    "Trial and Error: DBSCAN parameter selection often involves an iterative process of trial and error. Start with some initial values and examine the clusters. Adjust the parameters and repeat the process until satisfactory results are obtained.\n",
    "\n",
    "Remember that the choice of parameter values can have a significant impact on the clustering results. It is important to balance capturing meaningful clusters with avoiding over or under-segmentation. Also, keep in mind that the optimal parameter values may vary depending on the dataset and the specific problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fd9c4-6cec-4ef4-8de9-b4899f3a41be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e43f007-adff-4051-9989-04a0a22fac48",
   "metadata": {},
   "source": [
    "Q4. How does DBSCAN clustering handle outliers in a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13041a-894e-4e09-869a-cf5cd54471d1",
   "metadata": {},
   "source": [
    "ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that can handle outliers in a dataset in a specific manner. Here's how DBSCAN handles outliers:\n",
    "\n",
    "Density-based clustering: DBSCAN identifies clusters based on the density of data points in the feature space. It defines two important parameters: \"epsilon\" (ε) and \"minPts.\" Epsilon defines the maximum distance between two points for them to be considered as neighbors, and minPts specifies the minimum number of points required to form a dense region.\n",
    "\n",
    "Core points, border points, and noise: In DBSCAN, each data point is classified as either a core point, a border point, or a noise point (outlier) based on the density around it. A core point is a data point that has at least minPts data points (including itself) within its ε-neighborhood. Border points have fewer neighbors than minPts but are within the ε-neighborhood of a core point. Noise points are data points that are neither core points nor border points.\n",
    "\n",
    "Cluster formation: DBSCAN starts with an arbitrary core point and expands the cluster by connecting directly or indirectly density-reachable core points. It continues this process until it cannot find any more density-reachable points. Each cluster consists of core points and border points connected to the core points. Outliers (noise points) that do not belong to any cluster remain unassigned.\n",
    "\n",
    "Outlier detection: Outliers in DBSCAN are essentially the noise points that are not part of any cluster. They are considered to be points that do not satisfy the density requirements to be classified as core or border points. These points are not assigned to any cluster and are labeled as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fee485-52b7-4098-a2bd-9eebbf2e8133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705223a3-25bf-4204-be55-978b37b89c51",
   "metadata": {},
   "source": [
    "Q5. How does DBSCAN clustering differ from k-means clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58d3fd-6426-46bd-9f2e-45a8690fc4fd",
   "metadata": {},
   "source": [
    "ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and k-means clustering are two popular clustering algorithms, but they have fundamental differences in their approaches and characteristics. Here are some key differences between DBSCAN and k-means clustering:\n",
    "\n",
    "Cluster shape and flexibility:\n",
    "\n",
    "DBSCAN: DBSCAN can identify clusters of arbitrary shape. It is capable of finding clusters that are non-linear and have varying densities. It can handle clusters with irregular shapes, such as clusters with different densities, elongated clusters, or clusters with holes.\n",
    "k-means: k-means assumes that clusters are spherical and have equal variance. It tries to minimize the sum of squared distances between data points and the centroid of the cluster. As a result, it works well for globular, well-separated clusters of roughly equal size.\n",
    "Handling outliers:\n",
    "\n",
    "DBSCAN: DBSCAN has a built-in mechanism to handle outliers. It identifies noise points (outliers) that do not belong to any cluster based on their density. Outliers are not assigned to any cluster and are labeled as noise points.\n",
    "k-means: k-means does not explicitly handle outliers. Outliers can significantly affect the positions of cluster centroids and distort the clustering results. In k-means, outliers are likely to be assigned to the nearest centroid, even if they do not truly belong to any cluster.\n",
    "Number of clusters:\n",
    "\n",
    "DBSCAN: DBSCAN does not require specifying the number of clusters in advance. It automatically determines the number of clusters based on the density and connectivity of the data points. It can find clusters of varying sizes and shapes.\n",
    "k-means: k-means requires specifying the number of clusters (k) before running the algorithm. If the number of clusters is not known in advance, it may require iterative or heuristic approaches to determine the optimal k value.\n",
    "Input parameters:\n",
    "\n",
    "DBSCAN: DBSCAN has two main parameters: \"epsilon\" (ε) and \"minPts.\" Epsilon defines the maximum distance between two points for them to be considered as neighbors, and minPts specifies the minimum number of points required to form a dense region.\n",
    "k-means: k-means has a single parameter: the number of clusters (k). The algorithm aims to partition the data into k clusters based on minimizing the sum of squared distances.\n",
    "Complexity and scalability:\n",
    "\n",
    "DBSCAN: DBSCAN's time complexity is dependent on the number of data points and the density of the dataset. It can be more computationally expensive for large datasets, especially with high-dimensional data. However, it can efficiently handle large datasets when appropriate indexing structures are used.\n",
    "k-means: k-means is generally computationally efficient and can handle large datasets and high-dimensional data. However, its complexity can be affected by the number of data points, the number of clusters, and the number of iterations required for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1b7d8-e9c8-4ca3-a797-8658d76369b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b13ca6a-941f-422a-aed4-3bcb27c81903",
   "metadata": {},
   "source": [
    "Q6. Can DBSCAN clustering be applied to datasets with high dimensional feature spaces? If so, what are\n",
    "some potential challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47d321-0fc2-46cf-9848-80acb2989c58",
   "metadata": {},
   "source": [
    "ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density-based clustering algorithm that can be applied to datasets with high dimensional feature spaces. However, there are some potential challenges associated with using DBSCAN in high-dimensional spaces. Here are a few:\n",
    "\n",
    "Curse of dimensionality: In high-dimensional spaces, the curse of dimensionality becomes more pronounced. The data becomes increasingly sparse, and the notion of density becomes less meaningful. As a result, it becomes harder to define an appropriate neighborhood size or distance threshold for determining core samples and defining clusters effectively.\n",
    "\n",
    "Increased computational complexity: DBSCAN's computational complexity grows with the number of points and the dimensionality of the dataset. The distance calculation and neighborhood search become more computationally expensive in high-dimensional spaces, which can lead to performance issues, especially when dealing with large datasets.\n",
    "\n",
    "Concentration of points: In high-dimensional spaces, points tend to concentrate near the boundaries of the space rather than being evenly distributed. This concentration can cause the clustering algorithm to focus on the boundaries and neglect the interior regions, leading to suboptimal clustering results.\n",
    "\n",
    "Feature selection and dimensionality reduction: High-dimensional datasets often benefit from feature selection or dimensionality reduction techniques before applying clustering algorithms like DBSCAN. Removing irrelevant or redundant features can help mitigate the curse of dimensionality, improve clustering performance, and enhance the interpretability of the results.\n",
    "\n",
    "Parameter sensitivity: DBSCAN has two important parameters: epsilon (ε), which defines the neighborhood size, and min_samples, which determines the minimum number of points required to form a dense region. Choosing appropriate values for these parameters becomes more challenging in high-dimensional spaces. The parameters need to be carefully tuned to avoid overfitting or underfitting the data.\n",
    "\n",
    "To address these challenges, it is recommended to perform feature selection or dimensionality reduction techniques before applying DBSCAN to high-dimensional datasets. Additionally, alternative clustering algorithms designed specifically for high-dimensional data, such as subspace clustering or density-ratio-based methods, may be more suitable and provide better results in such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f0217-02b0-4907-a772-4b4bcf5ad8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36fd62a4-5350-4ef0-b81a-6c0210cde6c0",
   "metadata": {},
   "source": [
    "Q7. How does DBSCAN clustering handle clusters with varying densities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373be94d-3027-4cfc-bb95-a78f0ed91fd6",
   "metadata": {},
   "source": [
    "ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density-based clustering algorithm that can handle clusters with varying densities. Unlike some other clustering algorithms, DBSCAN does not assume that clusters have a specific shape or density distribution. Instead, it identifies dense regions of points separated by regions of lower density.\n",
    "\n",
    "DBSCAN works by defining two key parameters: epsilon (ε), which specifies the maximum distance between two points for them to be considered neighbors, and minPts, which sets the minimum number of points required to form a dense region (core point).\n",
    "\n",
    "Here's how DBSCAN handles clusters with varying densities:\n",
    "\n",
    "Core Points: DBSCAN starts by randomly selecting an unvisited data point and retrieves its ε-neighborhood (including the point itself). If the number of points in this neighborhood is equal to or greater than minPts, the point is labeled as a core point. Core points are the densest parts of clusters and serve as the starting points for cluster expansion.\n",
    "\n",
    "Directly Density-Reachable: DBSCAN expands the cluster by iteratively visiting the ε-neighborhood of each core point, considering each point in the neighborhood as part of the same cluster. If a point in the neighborhood is also a core point, its ε-neighborhood is further explored, and the process continues. This allows the algorithm to capture dense areas of any shape and size.\n",
    "\n",
    "Density-Connected: DBSCAN continues expanding clusters until there are no more directly density-reachable points. However, there may be points that are not core points but lie within the ε-neighborhood of a core point. These points are considered part of the same cluster but are not as dense. They are called density-connected points and can bridge gaps between clusters with different densities.\n",
    "\n",
    "Noise Points: Points that do not meet the criteria to be core or density-connected points are labeled as noise points or outliers. These points do not belong to any cluster and may be present in low-density regions or far from any cluster.\n",
    "\n",
    "By using the concept of density and connectivity, DBSCAN can naturally handle clusters with varying densities. It can discover clusters of different sizes, shapes, and densities, adjusting to the local characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db6d92-da21-40ae-a3a5-5d063e2dcf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4a0f2c-dc12-497d-a84c-eb66bb1e6200",
   "metadata": {},
   "source": [
    "Q8. What are some common evaluation metrics used to assess the quality of DBSCAN clustering results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64eb4d0-6088-4d9b-b934-04b2caa09feb",
   "metadata": {},
   "source": [
    "ans - When evaluating the quality of DBSCAN clustering results, there are several common evaluation metrics that can be used. Here are a few simple explanations of these metrics:\n",
    "\n",
    "Silhouette Coefficient: This metric measures how well each data point fits into its assigned cluster compared to other clusters. It ranges from -1 to 1, where a value close to 1 indicates a well-clustered data point, while values close to 0 or negative values suggest poor clustering.\n",
    "\n",
    "Davies-Bouldin Index: It quantifies the average similarity between clusters, where a lower value indicates better clustering. It measures the ratio of the average distance between clusters to the average intra-cluster distance.\n",
    "\n",
    "Calinski-Harabasz Index: This index evaluates the ratio of between-cluster dispersion to within-cluster dispersion. A higher value indicates better separation between clusters and hence better clustering.\n",
    "\n",
    "Adjusted Rand Index (ARI): This metric compares the clustering results against a known ground truth (if available) and measures the similarity between them. It returns a value between -1 and 1, where 1 indicates identical clustering and 0 represents random clustering.\n",
    "\n",
    "Fowlkes-Mallows Index (FMI): Similar to ARI, FMI also compares the clustering results to a known ground truth. It calculates the geometric mean of precision and recall, which indicates how well the clustering captures the true cluster assignments.\n",
    "\n",
    "These metrics provide different perspectives on the quality of clustering results, considering aspects such as compactness, separation, and agreement with known information (if available). Choosing the appropriate metric depends on the specific requirements and characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ca1df-127e-4d53-a384-5de4a58ce7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9869844-2d4f-4163-8799-9e394b47923e",
   "metadata": {},
   "source": [
    "Q9. Can DBSCAN clustering be used for semi-supervised learning tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca725586-a518-4697-aa82-62a9f2c56720",
   "metadata": {},
   "source": [
    "ans - No, DBSCAN clustering is not typically used for semi-supervised learning tasks. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an unsupervised learning algorithm that identifies clusters in data based on density. It does not require any labeled data or prior knowledge of the classes.\n",
    "\n",
    "Semi-supervised learning, on the other hand, is a combination of supervised and unsupervised learning, where a small portion of the data is labeled, and the remaining data is unlabeled. The goal is to leverage both the labeled and unlabeled data to improve the performance of the learning model.\n",
    "\n",
    "While DBSCAN can provide valuable insights into the structure of the data and identify dense regions, it does not make use of labeled data or incorporate any supervised learning techniques. Therefore, it is not directly applicable to semi-supervised learning tasks.\n",
    "\n",
    "In semi-supervised learning, other algorithms such as self-training, co-training, or generative models like Expectation-Maximization (EM) are commonly used to utilize both labeled and unlabeled data to improve the model's performance. These methods incorporate the available labeled data and leverage the unlabeled data to guide the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943dea4-190b-43ed-a4b5-88742e737343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7680d83-2783-42df-9410-f9e1fe6b7c15",
   "metadata": {},
   "source": [
    "Q10. How does DBSCAN clustering handle datasets with noise or missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b33f67-5d4b-44f3-bb07-826ad731dd68",
   "metadata": {},
   "source": [
    "ans - DBSCAN clustering can handle datasets with noise or missing values in a relatively straightforward manner:\n",
    "\n",
    "Noise: DBSCAN is designed to handle noise effectively. It defines clusters based on the density of data points, ignoring isolated points or outliers that do not belong to any cluster. These noisy points are considered as outliers or noise by the algorithm and are not assigned to any cluster. DBSCAN identifies clusters based on the density of neighboring points, so isolated noisy points will not significantly affect the clustering results.\n",
    "\n",
    "Missing Values: DBSCAN does not handle missing values directly. If a dataset contains missing values, they need to be handled beforehand by imputing or removing them. Imputation involves estimating the missing values based on the available data, while removal means eliminating the instances with missing values from the dataset. Once the missing values have been handled appropriately, DBSCAN can be applied to the complete dataset.\n",
    "\n",
    "It's worth noting that the effectiveness of DBSCAN on datasets with missing values depends on the nature and amount of missing data, as well as the imputation or removal strategy used. It's important to pre-process the dataset carefully to ensure meaningful and accurate clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95255d-52a0-4f92-86df-6cb78f709fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caef8a78-8b8e-421b-bd7a-2378835c2ec5",
   "metadata": {},
   "source": [
    "Q11. Implement the DBSCAN algorithm using a python programming language, and apply it to a sample\n",
    "dataset. Discuss the clustering results and interpret the meaning of the obtained clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a116e6f-9668-4a1b-9f1f-156fe2317d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [-1 -1 -1 -1  0 -1  0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1, 1], [1.5, 2], [3, 4], [5, 7], [3.5, 5], [4.5, 5], [3.5, 4.5]])\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# Print cluster labels\n",
    "print(\"Cluster labels:\", clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b92ab0-40d3-46c9-8e6f-d23f75677bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
